{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE152B - Spring 2023: Homework 2\n",
    "## Computing resources\n",
    "\n",
    "You should be assigned an account that has access to GPU clusters on https://datahub.ucsd.edu/. Steps to set up the environment:\n",
    "- Login with your UCSD credentials;\n",
    "- Launch a GPU instance with (1 GPU, 8 CPU, 16G RAM). This will lead you to a Jupyter Notebook GUI, where you can browse files, launch a terminal with bash environment via (upper-right) New -> Terminal.\n",
    "- You can also access the container with command line from your local terminal (see README.md for more).\n",
    "    \n",
    "<!-- ## Prepare the notebook and codes\n",
    "Once you are in the bash environment from the terminal, pull the code for HW1 by: -->\n",
    "\n",
    "<!-- # ```bash\n",
    "# cd ~\n",
    "# git clone --recurse-submodules -j8 https://github.com/ViLab-UCSD/cse152b-hw3-release-sp22.git\n",
    "# cd cse152b-hw3-release-sp22\n",
    "# ``` -->\n",
    "<!-- \n",
    "### Only for the first time\n",
    "Then prepare the environment only for the first time:\n",
    "```bash\n",
    "conda init bash # initialize the enviroment\n",
    "source ~/.bashrc\n",
    "conda activate /datasets/cs152b-sp22-a00-public/envs/cse152b-hw3-py38\n",
    "python -m ipykernel install --user --name cse152b-hw3-py38 --display-name \"Python (cse152b-hw3-py38)\" # make the public kernel accessible to your conda\n",
    "```\n",
    "\n",
    "Close this notebook and re-launch it. You should be able to switch to the kernel from the public environment for this homework, like this for example:\n",
    "\n",
    "![](images/kernel.png)\n",
    "\n",
    "If it works, you should see the environment name `\"Python (cse152b-hw3-py38)\"` on the upper-right corner as highlighted above.\n",
    "\n",
    "### Everytime later\n",
    "After this, everytime you use the terminal (newly launched/reconnected), run this so that you have access to all the packages:\n",
    "```bash\n",
    "conda activate /datasets/cs152b-sp22-a00-public/envs/cse152b-hw3-py38 # activate the enviroment\n",
    "```\n",
    "\n",
    "If it runs successfully, you will see prefix `(cse152b-hw3-py38)` in your current line similar to highlighted, like this for example:\n",
    "\n",
    "![](images/conda.png) -->\n",
    "\n",
    "## Extra instructions on the cluster/container/session managers\n",
    "Please refer to the README file for Section: **Extra Instructions**\n",
    "\n",
    "\n",
    "## Submission instructions\n",
    "1. Attempt all questions.\n",
    "2. Please comment all your code adequately.\n",
    "3. Include all relevant information such as text answers, output images in notebook.\n",
    "4. **Academic integrity:** The homework must be completed individually.\n",
    "\n",
    "5. **Submission instructions:**  \n",
    " (a) Submit the notebook and its PDF version on Gradescope, via:\n",
    "     - Option 1: Ctrl + P -> Save as PDF (toggling Headers and footers, Background graphics)\n",
    "     \n",
    " (b) Rename your submission files as Lastname_Firstname.ipynb and Lastname_Firstname.pdf.  \n",
    " (c) Correctly select pages for each answer (only your answers; excluding the problem description text) on Gradescope to allow proper grading.\n",
    "\n",
    "6. **Due date:** Assignments are due **Fri, May 26, by 11pm PST**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Using SphereFace [3] for Face Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In the first section, we will test a pretrained model of SphereFace on LFW [4] dataset. The [LFW dataset](http://vis-www.cs.umass.edu/lfw/#download) is on `../public/datasets/lfw`. The dataset contains 6000 pairs of human face images with ground truth labels for whether they are from the same identity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer here.``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The PyTorch code of SphereFace is located in `./sphereFace`,  which is modified based on the open source code from `https://github.com/clcarwin/sphereface_pytorch`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer here.``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Run the following commands and report the accuracy of SphereFace on LFW verification. **(5 points)**\n",
    "```\n",
    "cd sphereFace\n",
    "tar -zxf model.tar.gz\n",
    "python lfw_eval.py --model ./model/sphere20a_20171020.pth --net faceNet --lfw ../../public/datasets/lfw/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer here.``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Explain briefly how the following steps are performed when evaluating on LFW dataset: \n",
    "    1. Given the features extracted from the network, what is the metric used to measure the distance between two faces? (`lfw_eval.py`: Line 135) **(5 points)**\n",
    "    2. How is the threshold set to determine whether two faces are from the same identity? How is the accuracy computed? (`lfw_eval.py`: Line 141 to 148) **(10 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer here.``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. An important step before face recognition is face alignment, in which we warp and crop the image based on the location of facial landmarks.\n",
    "    1. Briefly describe how we warp and crop the image. (`lfw_eval.py`: Line 11-26) **(5 points)**\n",
    "    2. Instead of doing face alignment, crop an image patch of height 112 pixels and width 96 pixels at the center of the image. Report the accuracy.  **(10 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer here.``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Using MTCNN [5] for Detecting Face Landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Instead of using provided facial landmarks, we will now use MTCNN [5] for detecting them. The code is located at `./mtcnn`. Run the following commands to generate the facial landmarks. Include two example outputs in your report. **(5 points)**\n",
    "```\n",
    "cd mtcnn\n",
    "python lfw_landmark.py --lfw ../../public/datasets/lfw/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer here.``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Go to the `sphereFace` directory, run the following commands by setting flag `alignmentMode` to be 2. Report the error using the predicted facial landmarks. **(5 points)**\n",
    "```\n",
    "cd sphereFace\n",
    "python lfw_eval.py --model ./model/sphere20a_20171020.pth --net faceNet --lfw ../../public/datasets/lfw/ --alignmentMode 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer here.``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Next, answer the following questions:\n",
    "    1. Is the result better than using the landmarks provided in the previous question? If not, how can you improve performance? **(5 points)**\n",
    "    2.  What are the steps adopted by the method to achieve real-time speed? **(5 points)**\n",
    "    3. Briefly describe how non-maximal suppression (NMS) is implemented in this method. (`src/box_utils.py`: Line 5-68) **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer here.``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: Training CosFace [7] on CASIA Dataset [6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you are required to implement CosFace based on the code you use in the previous section, train it on CASIA dataset and test on LFW dataset. \n",
    "1. You will now train CosFace [7] on the CASIA dataset [6] and test on the LFW dataset [4]. In this section, the skeleton code for training is given. CASIA-Webface dataset can be found at `../../public/datasets/CASIA-WebFace`.  \n",
    "\n",
    "Go to directory `./cosFace` and open `faceNet.py`. Under class `CustomLinear` implement function $\\psi(\\theta_{y_i}, i)  = \\cos(\\theta_{y_i}, i) - m$ which is the cosine distance with margin $m$.\n",
    "Under class `CustomLoss`, implement the loss function with the returned $\\psi(\\theta_{y_i}, i)$ and $\\cos(\\theta_{j}, i)$. You may check (and duly cite) any open source implementation for hints on improving the performance. **(15 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Copy your implementation of CustomLinear and CustomLoss here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The original architecture of FaceNet in ``cosFace/FaceNet.py`` is a 20-layer residual network as described in Table 2 of [3], but without batch normalization. Now add batch normalization after every convolutional and fully connected layer. Train the new network on CASIA dataset and test on LFW dataset. You are free to change any hyper parameters but report the hyper parameters that you think might influence the performance. Following is a demonstration of a residual block with 128 filters and kernel size $3\\times3$:\n",
    "\\begin{align}\n",
    "y &=& \\mathtt{CONV}_{3\\times3, 128}(x) \\\\\n",
    "y &=& \\mathtt{BatchNorm}(y) \\\\\n",
    "y &=& \\mathtt{PReLU}(y) \\\\\n",
    "y &=& \\mathtt{CONV}_{3\\times3, 128}(y) \\\\\n",
    "y &=& \\mathtt{BatchNorm}(y) \\\\\n",
    "y &=& \\mathtt{PReLU}(y) \\\\\n",
    "\\mathtt{OUT} &=& x + y\n",
    "\\end{align}\n",
    "    1. Draw the training curves for accuracy and loss on CASIA and compare to the curve without batch normalization. **(10 points)**\n",
    "    2. Report accuracy on the LFW dataset, evaluated using `lfw_eval.py`. **(10 points)**\n",
    "    3. Do you achieve better performance on LFW? If yes, explain how batch normalization helps. If not, try to explain why the results are worse.  **(10 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer here.``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. If you achieve better performance compared to SphereFace in Q1, well done! Can you provide a reason? If you do not outperform SphereFace, can you provide a cause? **(10 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer here.``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Randomly choose 10 identities from the CASIA dataset, forward pass all their images through the CosFace network and visualize the normalized features (embeddings) using tSNE [8]. You can use code from https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html for visualization. Try a few random samples and include the figure that you consider most illustrative of the method. **(10 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer here.``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4: Human Pose Estimation with Convolutional Pose Machines (CPM) [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, you will be given code adapted from [PyTorch implementation of one of the CPM model [1]](https://github.com/Hzzone/pytorch-openpose), which is an follow-up work of the original Convolution Pose Machine [2], and shares the Part Confidence Maps estimation module with [2]. In this question you will be given a trained CPM model and gain insights about the model design and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run the inference code on the given image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0, 'pytorch-openpose')\n",
    "\n",
    "from src import model\n",
    "from src import util\n",
    "from src.body import Body\n",
    "from src.hand import Hand\n",
    "\n",
    "body_estimation = Body('../public/pytorch-openpose/model/body_pose_model.pth')\n",
    "\n",
    "test_image = 'pytorch-openpose/images/demo.jpg'\n",
    "oriImg = cv2.imread(test_image)  # B,G,R order\n",
    "candidate, subset, heatmap_list, heatmap_list_converted_list = body_estimation(oriImg)\n",
    "heatmap_0 = heatmap_list[0] \n",
    "canvas = copy.deepcopy(oriImg)\n",
    "canvas = util.draw_bodypose(canvas, candidate, subset)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(canvas[:, :, \n",
    "                  [2, 1, 0]])\n",
    "# plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Visualize the output keypoint detection result. **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Paste the output figure here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) What is the ratio of size $\\lambda=H/H'$ between the input image **im** (Line 57 of ``body.py``) \n",
    "of shape [1, 3, H, W] and the output heatmap **heatmap_0** of shape [1, D, H', W']? **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer the question here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) What module in the model is reponsible for this scaling? **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer the question here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ``heatmap_list_converted_list[0]`` is a list of heatmaps from all 6 layers: [out1_2, out2_2, out3_2, out4_2, out5_2, out6_2], where the output of each layer is of shape [H, W, D]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This snippet gives the function to visualize the $d_{th}$ feature map from the layer **layer_idx**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_idx = -1\n",
    "\n",
    "for d in range(heatmap_list_converted_list[0][layer_idx].shape[2]):\n",
    "    layer_idx = 0\n",
    "    heatmap = heatmap_list_converted_list[0][layer_idx][:, :, d]\n",
    "    util.overlay_heatmap(canvas, heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) What is D? And given D can you tell what is the number of keypoints that the model is trying to estimate? **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Visualize and compare the heatmap from **layer_idx=0** and **layer_idx=5** for the keypoint **d=2**. **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Paste and compare the visualizations here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Visualize and compare the heatmap from **layer_idx=0** and **layer_idx=5** for the keypoint **d=3**. **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Paste and compare the visualizations here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) State and justify the shared difference in these comparisons? You may get some hint from the model design. **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``State and justify the difference here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Set **multi_scale=True** in Line 38 of ``body.py`` and re-run the code from Q4(1). \n",
    "\n",
    "    (a) Visualize the output below. **(5 points)**\n",
    "    \n",
    "    **NOTE: you might need to restart the kernel each time you change the ``multi_scale`` flag. This may be required to re-initialize the model. [Kernel]-->[Restart]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Paste figure by re-running (a) with multi_scale=False here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Do the same comparison with additional images of ``demo_coco.jpg``, ``demo_coco2.jpg``, ``demo_beach.jpg``, ``demo_ucsd.jpg``, ``demo_ucsd2.jpg``.  **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``For each of the additional images, paste two figures with multi_scale=True and multi_scale=False here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) What shared difference can you spot in most of the comparisons? Justify the difference. **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Report the difference and jsutify here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "[1] Cao, Zhe. et al. \"Realtime multi-person 2d pose estimation using part affinity fields.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.\n",
    "\n",
    "[2] Wei, Shih-En. et al. \"Convolutional pose machines.\" Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. 2016.\n",
    "\n",
    "[3] Liu, Weiyang. et al. \"SphereFace: Deep Hypersphere Embedding for Face Recognition.\" arXiv:1704.08063.\n",
    "\n",
    "[4] Huang, Gary. et al. \"Labeled faces in the wild:  Adatabase for studying face recognition in unconstrained environments.\" Technical Report 07-49, Universityof Massachusetts, Amherst, October 2007.\n",
    "\n",
    "[5] Zhang, Kaipeng. et al. \"Joint face detection and alignment usingmultitask cascaded convolutional networks.\" IEEE Signal Processing Letters, 23(10):1499–1503, 2016.\n",
    "\n",
    "[6] Yi, Dong. et al.  Learning face representation from scratch. arXiv:1411.7923.\n",
    "\n",
    "[7] Wang, Hao. et al.Cosface: Large margin cosine loss for deep face recognition. In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition, pages 5265–5274, 2018.\n",
    "\n",
    "[8] Maaten, Laurens van der, and Geoffrey Hinton. \"Visualizing data using t-SNE.\" Journal of machine learning research 9.Nov (2008): 2579-2605."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
